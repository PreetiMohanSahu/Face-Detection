{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceClassifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "img = cv2.imread(\"indian.jpg\")\n",
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "faces = faceClassifier.detectMultiScale(gray_image, 1.3, 12)\n",
    "for x,y,w,h in faces:\n",
    "    img = cv2.rectangle(img, (x,y), (x + w, y + h), (255,255,255), 3)\n",
    "while True:\n",
    "    cv2.imshow(\"My Face - New\", img)\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    Con, img = cam.read()\n",
    "    if Con:\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "        faces = faceClassifier.detectMultiScale(gray_image, 1.3, 6)\n",
    "        for x,y,w,h in faces:\n",
    "            img = cv2.rectangle(img, (x,y), (x + w, y + h), (0,255,0), 3)\n",
    "        cv2.imshow(\"My Face - New\", img)\n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "    else:\n",
    "        print(\"Connect Your Camera..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Collected Your Faces...\n"
     ]
    }
   ],
   "source": [
    "def CreateDataSet():\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    Sam = 0\n",
    "    while True:\n",
    "        Con, img = cam.read()\n",
    "        if Con:\n",
    "            gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = faceClassifier.detectMultiScale(gray_image, 1.3, 6)\n",
    "            for x,y,w,h in faces:\n",
    "                face = gray_image[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, (200, 200))\n",
    "                \n",
    "                cv2.imwrite(\"Images/user.{}.jpg\".format(Sam), face)\n",
    "                Sam = Sam + 1\n",
    "                face = cv2.putText(face, str(Sam), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "                cv2.imshow(\"My Face\", face)\n",
    "            \n",
    "            \n",
    "            if Sam == 50:\n",
    "                print(\"We Collected Your Faces...\")\n",
    "                break\n",
    "                \n",
    "            if cv2.waitKey(1) == 13:\n",
    "                break\n",
    "    cam.release()\n",
    "\n",
    "CreateDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Our Model With Your Faces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = \"Images/\"\n",
    "all_images = os.listdir(path)[:-1]\n",
    "\n",
    "Training_Data = []\n",
    "Labels = np.arange(1, len(all_images)+1)\n",
    "Labels = np.asarray(Labels, dtype = np.int32)\n",
    "for i in all_images:\n",
    "    img_path = path + i\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Narray = np.asarray(image, dtype = np.uint8)\n",
    "    Training_Data.append(Narray)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Your Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Trained Your Machine with Your Faces.\n"
     ]
    }
   ],
   "source": [
    "#Face_Model = cv2.face.LBPHFaceRecognizer_create()\n",
    "Face_Model = cv2.face_LBPHFaceRecognizer.create()\n",
    "Face_Model.train(Training_Data, Labels)\n",
    "print(\"We Trained Your Machine with Your Faces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Detect Face...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 24.49585473325188)\n",
      "(194, 24.22418791415784)\n",
      "(16, 34.94160175944394)\n",
      "(214, 29.12671935560957)\n",
      "(268, 35.437061624837256)\n",
      "(58, 40.552242479703814)\n",
      "(18, 34.65460709356856)\n",
      "(289, 116.10195725130575)\n",
      "(253, 35.54565088561947)\n",
      "(24, 115.7848384251449)\n",
      "(154, 34.88471933828728)\n",
      "(24, 114.35568102691852)\n",
      "(57, 35.06655104518076)\n",
      "(289, 108.0590151131777)\n",
      "(154, 33.69873715613154)\n",
      "(158, 118.41606801705393)\n",
      "(239, 33.65297714232929)\n",
      "(24, 114.1923225373261)\n",
      "(289, 109.84115692271038)\n",
      "(289, 104.55295905100036)\n",
      "(158, 109.80345351747884)\n",
      "(24, 110.92863428019808)\n",
      "(289, 70.43223433905412)\n",
      "(289, 77.55852422774998)\n",
      "(289, 78.91797191003509)\n",
      "(289, 82.83062305480419)\n",
      "(289, 83.99758971603127)\n",
      "(172, 85.08992391587856)\n",
      "(289, 83.8846575159024)\n",
      "(199, 82.09175057722628)\n",
      "(289, 81.69961333334349)\n",
      "(289, 81.06057140484698)\n",
      "(289, 77.32136128689135)\n",
      "(289, 80.54757283239022)\n",
      "(2, 39.019106738194)\n",
      "(289, 78.7272713731441)\n",
      "(53, 36.87740124602109)\n",
      "(289, 78.08003549554053)\n",
      "(239, 32.51011742617632)\n",
      "(289, 110.67084663643536)\n",
      "(53, 36.693280711851415)\n",
      "(289, 114.74850080640383)\n",
      "(251, 33.47865622721783)\n",
      "(2, 41.52627967110199)\n",
      "(23, 37.00793247586914)\n",
      "(158, 111.67151704678068)\n",
      "(53, 36.48447263291596)\n",
      "(289, 106.00982307061236)\n",
      "(43, 36.079543784617755)\n",
      "(289, 81.66391456655224)\n",
      "(53, 37.09018271459131)\n",
      "(289, 78.88775469610427)\n",
      "(61, 35.77440449143783)\n",
      "(289, 78.09021455148357)\n",
      "(24, 81.134026522356)\n",
      "(268, 40.20671093198666)\n",
      "(194, 38.43354593011538)\n",
      "(23, 32.84282021742494)\n",
      "(255, 34.565704761250615)\n",
      "(24, 100.27784239357341)\n",
      "(20, 31.007773062614344)\n",
      "(24, 102.7735538269197)\n",
      "(19, 32.04797063433138)\n",
      "(289, 68.40687826648723)\n",
      "(53, 30.75262270970277)\n",
      "(289, 69.2790844152295)\n",
      "(53, 30.235591581427926)\n",
      "(289, 71.93960035632662)\n",
      "(53, 31.86890763194276)\n",
      "(75, 31.171229325729673)\n",
      "(161, 29.818255830057883)\n",
      "(24, 114.64330366187588)\n",
      "(24, 88.83571157201905)\n",
      "(24, 96.19995699914037)\n",
      "(268, 33.6056591394658)\n",
      "(289, 90.07855471147695)\n",
      "(161, 30.455913613959044)\n",
      "(289, 109.24938216579332)\n",
      "(161, 30.487789643669135)\n",
      "(24, 114.77355881539178)\n",
      "(139, 30.126448747390356)\n",
      "(56, 110.54545648101445)\n",
      "(154, 31.209355846570546)\n",
      "(56, 111.24922617378206)\n",
      "(194, 32.72653392257455)\n",
      "(158, 111.69907305131306)\n",
      "(158, 110.02023026135886)\n",
      "(158, 111.42944531232837)\n",
      "(56, 104.78630017111004)\n",
      "(24, 109.08607206098252)\n",
      "(158, 109.05311828245846)\n",
      "(289, 104.96237619581325)\n",
      "(156, 110.99468771895269)\n",
      "(56, 94.4057943936436)\n",
      "(289, 64.49148799221746)\n",
      "(289, 68.17886158510959)\n",
      "(289, 68.21437265075004)\n",
      "(289, 66.67033999204808)\n",
      "(268, 32.84256153668581)\n",
      "(289, 64.31496001444563)\n",
      "(19, 34.047522705199135)\n",
      "(289, 69.18213913186133)\n",
      "(53, 32.63837389345696)\n",
      "(289, 65.11384538044588)\n",
      "(53, 34.18287535481843)\n",
      "(289, 74.7504198070941)\n",
      "(24, 84.49076745664112)\n",
      "(24, 84.83327325640896)\n",
      "(24, 85.85044197055349)\n",
      "(24, 80.15101571937055)\n",
      "(24, 78.86326371175592)\n",
      "(20, 33.774345681393754)\n",
      "(43, 31.22323179864946)\n",
      "(53, 31.013253908165694)\n",
      "(253, 104.41725779118013)\n",
      "(53, 29.631899292495444)\n",
      "(289, 103.62918313258163)\n",
      "(197, 108.81498329763522)\n",
      "(158, 106.79599220012628)\n",
      "(56, 107.74595611294424)\n",
      "(158, 103.5113210427096)\n",
      "(197, 99.14982892761535)\n",
      "(20, 37.610628670561645)\n",
      "(289, 95.57144669608138)\n",
      "(24, 75.07933740194294)\n",
      "(269, 32.49829250237731)\n",
      "(289, 103.63958913281995)\n",
      "(161, 32.046372691466)\n",
      "(56, 111.35012911391796)\n",
      "(253, 112.34932508463582)\n",
      "(24, 95.03431573114992)\n",
      "(66, 83.11361749359787)\n",
      "(24, 87.0906341157614)\n",
      "(24, 73.21654757518915)\n",
      "(24, 66.5928380316135)\n",
      "(24, 61.875841528019244)\n",
      "(24, 64.03566330937811)\n",
      "(24, 65.19875823201144)\n",
      "(24, 75.19812302280721)\n",
      "(24, 67.77479206513617)\n",
      "(268, 43.307294849873735)\n",
      "(20, 41.49279291274291)\n",
      "(19, 32.47778704800979)\n",
      "(53, 33.99298544987656)\n",
      "(214, 33.40726008014907)\n",
      "(24, 95.93075017271012)\n",
      "(24, 91.30000932469903)\n",
      "(24, 91.79526765438298)\n",
      "(24, 91.68023923947649)\n",
      "(58, 40.03770091601209)\n",
      "(24, 71.66811271950223)\n",
      "(238, 38.332903500792554)\n",
      "(24, 96.24112788041174)\n",
      "(24, 77.04219729712926)\n",
      "(24, 111.27071546432472)\n",
      "(24, 112.33207851152507)\n",
      "(24, 93.08390308440691)\n",
      "(20, 35.68666660136333)\n",
      "(20, 37.439600379141424)\n",
      "(24, 102.43419880867496)\n",
      "(24, 92.60525948489145)\n",
      "(24, 80.0892855141475)\n",
      "(24, 81.39114936243266)\n",
      "(24, 80.24817143115742)\n",
      "(24, 92.0837479827609)\n",
      "(24, 95.42592607682737)\n",
      "(24, 88.49322541380262)\n",
      "(24, 92.77591709888976)\n",
      "(24, 90.8113218431339)\n",
      "(24, 89.84436548140707)\n",
      "(24, 87.44587848601186)\n",
      "(24, 81.66794076286398)\n",
      "(24, 84.19410489187285)\n",
      "(24, 89.16282848509115)\n",
      "(20, 32.61424413083826)\n",
      "(2, 37.51123206943618)\n",
      "(23, 53.26875868329966)\n",
      "(2, 41.16332180239367)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    Con, img = cam.read()\n",
    "    if Con:\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = faceClassifier.detectMultiScale(gray_image, 1.3, 6)\n",
    "        for x,y,w,h in faces:\n",
    "            face = gray_image[y:y+h, x:x+w]\n",
    "            face = cv2.resize(face, (200, 200))\n",
    "\n",
    "            pred = Face_Model.predict(face)\n",
    "            print(pred)\n",
    "            if pred[1]< 42:\n",
    "                face = cv2.putText(img, \"Hey! Preeti Mohan\", (x, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "                subprocess.call([\"say\", \"Hey Preeti Mohan. How Can i Help You..\"])\n",
    "            else:\n",
    "                face = cv2.putText(img, \"Priyabrata @\", (x, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "                \n",
    "           \n",
    "        cv2.imshow(\"MyFace\", img)\n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = cv2.imread(\"Images/user.11.jpg\")\n",
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceClassifier.detectMultiScale(gray_image, 1.3, 6)\n",
    "for x,y,w,h in faces:\n",
    "    face = gray_image[y:y+h, x:x+w]\n",
    "    face = cv2.resize(face, (200, 200))\n",
    "\n",
    "    pred = Face_Model.predict(face)\n",
    "    print(pred)\n",
    "    if pred[1]< 42\n",
    "        face = cv2.putText(img, \"Hey! Preeti Mohan\", (x, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        subprocess.call([\"say\", \"Hey! Preeti Mohan. How Can i Help You..\"])\n",
    "    else:\n",
    "        face = cv2.putText(img, \"Priyabrata @\", (x, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "\n",
    "\n",
    "cv2.imshow(\"MyFace\", img)\n",
    "if cv2.waitKey(1) == 13:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
